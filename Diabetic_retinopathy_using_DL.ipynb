{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80_wYbSjkUIZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dense, Dropout, Lambda, multiply\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "base_image_dir = os.path.join('.', 'C:/Users/Sarang/Documents/Final Year Project/train/train')\n",
        "df = pd.read_csv(os.path.join('C:/Users/Sarang/Documents/Final Year Project/train/trainLabels.csv'))\n",
        "df['path'] = df['image'].map(lambda x: os.path.join(base_image_dir, '{}.jpeg'.format(x)))\n",
        "df['exists'] = df['path'].map(os.path.exists)\n",
        "df = df[df['exists']]\n",
        "df = df.drop(columns=['image', 'exists'])\n",
        "df['level'] = df['level'].astype(str)"
      ],
      "metadata": {
        "id": "t1siqU5CkVZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Y7s_pT3tkV8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255\n",
        ")\n",
        "\n",
        "x_train = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    directory=\".\",\n",
        "    x_col=\"path\",\n",
        "    y_col=\"level\",\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "x_test = test_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    directory=\".\",\n",
        "    x_col=\"path\",\n",
        "    y_col=\"level\",\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "C-FUpc6ekWRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def build_model(input_shape, num_classes):\n",
        "    in_lay = Input(input_shape)\n",
        "\n",
        "    # Use InceptionV3 as the base model\n",
        "    base_model = InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "    # Extract features from the base model\n",
        "    pt_features = base_model(in_lay)\n",
        "\n",
        "    # Add Batch Normalization\n",
        "    bn_features = BatchNormalization()(pt_features)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attn_layer = Conv2D(64, kernel_size=(1, 1), padding='same', activation='relu')(bn_features)\n",
        "    attn_layer = Conv2D(16, kernel_size=(1, 1), padding='same', activation='relu')(attn_layer)\n",
        "    attn_layer = Conv2D(8, kernel_size=(1, 1), padding='same', activation='relu')(attn_layer)\n",
        "    attn_layer = Conv2D(1, kernel_size=(1, 1), padding='valid', activation='sigmoid')(attn_layer)\n",
        "\n",
        "    # Rescale attention weights\n",
        "    up_c2_w = np.ones((1, 1, 1, 2048))\n",
        "    up_c2 = Conv2D(2048, kernel_size=(1, 1), padding='same', activation='linear', use_bias=False, weights=[up_c2_w])\n",
        "    up_c2.trainable = False\n",
        "    attn_layer = up_c2(attn_layer)\n",
        "\n",
        "    # Apply attention to features\n",
        "    mask_features = multiply([attn_layer, bn_features])\n",
        "\n",
        "    # Global Average Pooling (GAP) instead of MaxPooling\n",
        "    gap_features = GlobalAveragePooling2D()(mask_features)\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Dense(512, activation='relu')(gap_features)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer\n",
        "    out_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[in_lay], outputs=[out_layer])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ol-y62gKkWa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = build_model(input_shape=(256, 256, 3), num_classes=x_train.num_classes)"
      ],
      "metadata": {
        "id": "E9aFdLSmkWjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "metadata": {
        "id": "2lVwxYIykWqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "filepath = \"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "earlystop = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=15, verbose=1, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=6, min_lr=0.001, verbose=1)"
      ],
      "metadata": {
        "id": "KeItqsSVkWxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [checkpoint, earlystop, reduce_lr]"
      ],
      "metadata": {
        "id": "WSuMdSUvkW2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    steps_per_epoch=x_train.samples // 32,\n",
        "    epochs=20,\n",
        "    validation_data=x_test,\n",
        "    validation_steps=x_test.samples // 32,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "92dNMiPnkW7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "train_loss, train_acc = model.evaluate(x_train)\n",
        "test_loss, test_acc = model.evaluate(x_test)\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc}\")\n",
        "print(f\"Validation Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "QTsbWPXkkzCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix and Classification Report\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "class_labels = x_test.class_indices\n",
        "class_labels = {v: k for k, v in class_labels.items()}\n",
        "\n",
        "cm = confusion_matrix(x_test.classes, y_pred)\n",
        "print('Confusion Matrix')\n",
        "print(cm)\n",
        "\n",
        "print('Classification Report')\n",
        "target_names = list(class_labels.values())\n",
        "print(classification_report(x_test.classes, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "fMjXsbtRk01d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(target_names))\n",
        "plt.xticks(tick_marks, target_names, rotation=90)\n",
        "plt.yticks(tick_marks, target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HkB94KHtk4Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"improved_diabetic_retinopathy_model.h5\")"
      ],
      "metadata": {
        "id": "1az8Iz3uk5v3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}